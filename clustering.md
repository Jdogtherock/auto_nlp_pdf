# Clustering Algorithms and Evaluation

## **1. Centroid-based Clustering: K-means**
- **Description**: K-means clustering aims to partition a set of observations into K clusters, where each observation belongs to the cluster with the nearest mean.

### **Evaluation for K-means**:
- **Metrics**:
  - **Silhouette Score**: Measures the similarity of a point to its own cluster compared to other clusters.
  - **Within-cluster Sum of Squares (WCSS)**: Sum of the squared distance between each point and the centroid of its cluster.
  - **Davies-Bouldin Index**: Measures the average similarity between clusters.

- **Visualizations**:
  - **Elbow Method**: Plot the number of clusters against the corresponding WCSS. The 'elbow' of the curve represents an optimal value for k (a balance between precision and computational cost).
  - **Silhouette Plot**: Visualization of each point's silhouette coefficient, giving insights into the separation distance between the resulting clusters.
  - **Cluster scatter plot**: Visual representation of clusters usually using the first two principal components.

---

## **2. Hierarchical clustering: Agglomerative clustering**
- **Description**: Hierarchical clustering builds nested clusters by merging or splitting them successively.

### **Evaluation for Agglomerative clustering**:
- **Metrics**:
  - **Cophenetic Correlation Coefficient**: Measures how faithfully the dendrogram preserves the pairwise distances between the original data points.

- **Visualizations**:
  - **Dendrogram**: Tree diagram to show the arrangement of the clusters produced.

---

## **3. Density-based clustering: DBSCAN**
- **Description**: DBSCAN groups together points that are close to each other based on a distance measurement and a minimum number of points.

### **Evaluation for DBSCAN**:
- **Metrics**:
  - **Silhouette Score**: As described above.

- **Visualizations**:
  - **Cluster scatter plot**: With noise points (those not belonging to any cluster) usually visualized in a different color.

---

## **4. Distribution-based clustering: Gaussian Mixture Model (GMM)**
- **Description**: GMM assumes that the data is generated from several Gaussian distributions. It tries to identify these distributions and cluster data accordingly.

### **Evaluation for GMM**:
- **Metrics**:
  - **Bayesian Information Criterion (BIC)**: Evaluates the likelihood of the data given the model.
  - **Akaike Information Criterion (AIC)**: Like BIC but places a different penalty on the complexity of the model.

- **Visualizations**:
  - **Probability Density Plots**: Visual representation of how the data is generated by different Gaussian distributions.
  - **Cluster scatter plot**: As described above.

---

## **5. Spectral clustering**
- **Description**: Spectral clustering uses the eigenvalues of the similarity matrix to reduce the dimensionality of the data before clustering in a lower-dimensional space.

### **Evaluation for Spectral clustering**:
- **Metrics**:
  - **Silhouette Score**: As described above.

- **Visualizations**:
  - **Cluster scatter plot**: On reduced dimensionality data.

---

## **6. Neural network-based clustering: Self Organizing Maps (SOM)**
- **Description**: SOMs transform a list of inputs into a two-dimensional map where similar inputs are grouped together.

### **Evaluation for SOM**:
- **Metrics**:
  - No typical internal metrics used for SOM. However, you can still utilize silhouette score if applicable.

- **Visualizations**:
  - **U-Matrix (Unified Distance Matrix)**: Represents the distance between adjacent neurons, helping to visualize cluster separation.
  - **Component Planes**: Displays the value each neuron has for a particular input feature.